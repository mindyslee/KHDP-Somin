{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_DIR      = \"/data\"      # your hand-made CSVs\n",
    "DATASETS_DIR  = \"/datasets\"  # KHDP provided data\n",
    "\n",
    "LABELS_CSV    = os.path.join(DATA_DIR, \"labels_with_hf.csv\")\n",
    "COMORB_CSV    = os.path.join(DATA_DIR, \"tabular.csv\")\n",
    "ECG_DIR       = os.path.join(DATASETS_DIR, \"ECG-Registry\", \"1.0.0\", \"1.MAIN\")\n",
    "\n",
    "VGPU_BATCH_SIZE = 32\n",
    "VGPU_EPOCHS     = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "comorb_df = pd.read_csv(COMORB_CSV, index_col=\"person_id\")\n",
    "feat_df  = comorb_df.drop(columns=[\"hf_outcome\"])\n",
    "\n",
    "# compute mean/std once\n",
    "mean_vals = feat_df.mean().values.astype(np.float32)\n",
    "std_vals  = feat_df.std().replace(0,1).values.astype(np.float32)\n",
    "\n",
    "comorb_map = {\n",
    "    pid: feat_df.loc[pid].values.astype(np.float32)\n",
    "    for pid in feat_df.index\n",
    "}\n",
    "\n",
    "print(f\"Loaded comorbidity for {len(comorb_map)} patients with {feat_df.shape[1]} features each\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ECGDataset(Dataset):\n",
    "def __init__(self, ecg_dir, records_df, transform=None):\n",
    "    self.ecg_dir   = ecg_dir\n",
    "    self.records   = records_df.reset_index(drop=True)\n",
    "    self.transform = transform\n",
    "\n",
    "def __len__(self):\n",
    "    return len(self.records)\n",
    "\n",
    "def __getitem__(self, idx):\n",
    "    row = self.records.iloc[idx]\n",
    "    path = os.path.join(self.ecg_dir, row.ecg_file)\n",
    "    # adjust loader to match your filetype\n",
    "    sig = np.load(path) if path.endswith(\".npy\") else torch.load(path)\n",
    "    sig = torch.tensor(sig, dtype=torch.float32)\n",
    "    if self.transform:\n",
    "        sig = self.transform(sig)\n",
    "    return sig, row.person_id, row.hf_outcome\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "def __init__(self, records_df, comorb_map, mean, std):\n",
    "    self.records = records_df.reset_index(drop=True)\n",
    "    self.comorb  = comorb_map\n",
    "    self.mean    = mean\n",
    "    self.std     = std\n",
    "\n",
    "def __len__(self):\n",
    "    return len(self.records)\n",
    "\n",
    "def __getitem__(self, idx):\n",
    "    row = self.records.iloc[idx]\n",
    "    pid = row.person_id\n",
    "    x   = self.comorb[pid]\n",
    "    x_norm = (x - self.mean) / self.std\n",
    "    return torch.tensor(x_norm, dtype=torch.float32), torch.tensor(row.hf_outcome, dtype=torch.float32)\n",
    "\n",
    "class CombinedDataset(Dataset):\n",
    "def __init__(self, ecg_ds, tab_ds):\n",
    "    assert len(ecg_ds) == len(tab_ds), \"ECG vs Tabular size mismatch!\"\n",
    "    self.ecg_ds = ecg_ds\n",
    "    self.tab_ds = tab_ds\n",
    "\n",
    "def __len__(self):\n",
    "    return len(self.ecg_ds)\n",
    "\n",
    "def __getitem__(self, idx):\n",
    "    sig, pid, y1 = self.ecg_ds[idx]\n",
    "    x_tab, y2    = self.tab_ds[idx]\n",
    "    return sig, x_tab, y1  # y1 == y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ECGEncoder(nn.Module):\n",
    "def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv1d(12,32,5,padding=2), nn.ReLU(),\n",
    "        nn.MaxPool1d(2),\n",
    "        nn.Conv1d(32,64,5,padding=2), nn.ReLU(),\n",
    "        nn.AdaptiveAvgPool1d(1)\n",
    "    )\n",
    "\n",
    "def forward(self, x):\n",
    "    return self.conv(x).view(x.size(0), -1)\n",
    "\n",
    "class TabularEncoder(nn.Module):\n",
    "def __init__(self, in_dim):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Linear(in_dim,64), nn.ReLU(),\n",
    "        nn.Linear(64,32),    nn.ReLU()\n",
    "    )\n",
    "\n",
    "def forward(self, x):\n",
    "    return self.net(x)\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "def __init__(self, tab_in_dim):\n",
    "    super().__init__()\n",
    "    self.ecg_enc  = ECGEncoder()\n",
    "    self.tab_enc  = TabularEncoder(tab_in_dim)\n",
    "    self.classif = nn.Sequential(\n",
    "        nn.Linear(64+32,32), nn.ReLU(),\n",
    "        nn.Linear(32,1)\n",
    "    )\n",
    "\n",
    "def forward(self, sig, tab):\n",
    "    e = self.ecg_enc(sig)\n",
    "    t = self.tab_enc(tab)\n",
    "    return self.classif(torch.cat([e,t], dim=1)).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "records = pd.read_csv(LABELS_CSV)\n",
    "uids             = records.person_id.unique()\n",
    "train_uids, val_uids = train_test_split(\n",
    "    uids,\n",
    "    test_size=0.2,\n",
    "    stratify=records.drop_duplicates(\"person_id\").hf_outcome,\n",
    "    random_state=42\n",
    ")\n",
    "train_df = records[records.person_id.isin(train_uids)].reset_index(drop=True)\n",
    "val_df   = records[records.person_id.isin(val_uids)].reset_index(drop=True)\n",
    "counts        = train_df.hf_outcome.value_counts()\n",
    "class_weights = {0:1/counts[0], 1:1/counts[1]}\n",
    "train_weights = train_df.hf_outcome.map(class_weights).values\n",
    "train_sampler = WeightedRandomSampler(train_weights, len(train_weights), replacement=True)\n",
    "\n",
    "print(f\"Train/Val patients: {len(train_uids)}/{len(val_uids)}, records: {len(train_df)}/{len(val_df)}\")\n",
    "print(f\"Class weights: {class_weights}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ecg_tr  = ECGDataset(ECG_DIR, train_df)\n",
    "ecg_val = ECGDataset(ECG_DIR, val_df)\n",
    "tab_tr  = TabularDataset(train_df, comorb_map, mean_vals, std_vals)\n",
    "tab_val = TabularDataset(val_df,   comorb_map, mean_vals, std_vals)\n",
    "ds_tr   = CombinedDataset(ecg_tr,  tab_tr)\n",
    "ds_val  = CombinedDataset(ecg_val, tab_val)\n",
    "\n",
    "loader_tr = DataLoader(ds_tr, batch_size=VGPU_BATCH_SIZE, sampler=train_sampler)\n",
    "loader_val= DataLoader(ds_val, batch_size=VGPU_BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model     = CombinedModel(tab_in_dim=len(mean_vals)).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(1, VGPU_EPOCHS+1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (sig, tab_x, y) in enumerate(loader_tr, 1):\n",
    "        sig, tab_x, y = sig.to(device), tab_x.to(device), y.to(device)\n",
    "        logits = model(sig, tab_x)\n",
    "        loss   = criterion(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * sig.size(0)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch}  Batch {i}/{len(loader_tr)}  Loss: {loss.item():.4f}\")\n",
    "    print(f\"→ Epoch {epoch}  TRAIN avg loss: {train_loss/len(loader_tr.dataset):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for sig, tab_x, y in loader_val:\n",
    "            sig, tab_x, y = sig.to(device), tab_x.to(device), y.to(device)\n",
    "            val_loss += criterion(model(sig, tab_x), y).item() * sig.size(0)\n",
    "    print(f\"→ Epoch {epoch}  VAL   avg loss: {val_loss/len(loader_val.dataset):.4f}\\n\")\n",
    "\n",
    "# save final model\n",
    "torch.save(model.state_dict(), \"hf_risk_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
